{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Cost Function\n",
    "\n",
    "For a given set of $m$ input/output pairs $(x_{i}, y_{i})$, we try to use some hypothesis function $h_{\\theta}$ to approximate $y_{i}$ given $x_{i}$.\n",
    "\n",
    "$$\n",
    "h_{\\theta}(x) = \\theta_{0} - \\theta_{1}x\n",
    "$$\n",
    "\n",
    "We then use this hypothesis function to derive our cost function, $J(\\theta_{0},\\theta_{1})$:\n",
    "\n",
    "$$\n",
    "J(\\theta_{0},\\theta_{1}) = \\min_{\\theta_{0},\\theta_{1}} \\frac{1}{2m}\\sum_{i=1}^{m} (h(x_{i}) - y_{i})^{2}\n",
    "$$\n",
    "\n",
    "So, we try to find the $\\theta_{0}$ and $\\theta_{1}$ which minimize the average squared error (multiplied by $\\frac{1}{2}$ to \"make the math easier\" according to Ng)  over the data set of $X, Y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Learning and Gradient Descent\n",
    "\n",
    "\n",
    "Process of gradient descent for can be described as repeating the following until convergence is reached:\n",
    "\n",
    "$$\n",
    "\\theta_{j} := \\theta_{j} - \\alpha \\frac{\\partial}{\\partial \\theta_{j}} J(\\theta_{0},\\theta_{1})\n",
    "$$\n",
    "\n",
    "- $\\alpha$ is the _learning rate_. We'll consider it a constant scalar for the moment.\n",
    "- Crucially, $\\theta_{0}...\\theta{n}$ must be updated simultaneously. Each $\\theta_{i}$ represents a different _feature_.\n",
    "- directionality of the \"step\" is determined by the derivative of $J$ w/r/t $\\theta_{j}$\n",
    "\n",
    "Conventiently, we can get convergence while leaving the _learning rate_ $\\alpha$ constant, because as we approach the local optimum, the magnitude of the slope will decrease, decreasing our step size.\n",
    "\n",
    "Next, we can use gradient descent to implement the minimization of our cost function as used in linear regression. So it seems like the $\\frac{1}{2}$ constant we multiplied the average squared error by in our linear regression function \"makes the math easier\" because we're differentiating a square function (i.e. the derivative of $\\frac{1}{2m}\\sum_{i=1}^{m} (h(x_{i}) - y_{i})^{2}$ is just $\\frac{1}{m}\\sum_{i=1}^{m} h(x_{i}) - y_{i}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra Review\n",
    "\n",
    "- Matrices are $n$ rows by $m$ columns\n",
    "- $A_{ij}$ expresses the element at the $i$th row and $j$th column of matrix $A$\n",
    "- no zero-indexing here. sad!\n",
    "- $\\mathbb{R}$ refers to set of real scalar values, $\\mathbb{R}^{n}$ refers to the set of all $n$-dimensional, real-valued vectors\n",
    "- addition/multiplication of matrices, vectors, and scalars is the same it's been in every single other math class you've already taken\n",
    "- an $m$ x $n$ matrix can only be multiplied by a $1$ x $m$ vector on its left or an $n$ x $1$ vector on its right.\n",
    "- similarly, an $n$ x $m$ matrix can only be multiplied on its left by a matrix of $p$ x $m$ on its left or a matrix of $n$ x $p$ on its right, where $p$ is an arbitrary natrual number.\n",
    "- matrix multiplication is not commutative, but it is associative\n",
    "- the identity matrix ($I$) has all $1$s down it's top-left to bottom-right diagonal and zeros otherwise; holds the property that any matrix $A$ multiplied by it (either on the left or right) is equal to itself, given that the dimensions are applicable.\n",
    "- the transpose of a matrix $A$ is denoted as $A^{T}$ and is simply a transposition of its rows and columns, i.e. if $A$ is $m$ x $n$, then $A^{T}$ is $n$ x $m$.\n",
    "- the inverse of a matrix $A$ is denoted as $A^{-1}$ and is expressed as $A^{-1}A = I$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
